# -*- coding: utf-8 -*-
"""Pico.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MCr2_Ei3jLIxyG5tZrGU1YE910q8OMWE
"""

# -*- coding: utf-8 -*-


# Install required packages
!pip install tiktoken datasets matplotlib tqdm

# Import necessary libraries
import time
import random
import math
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import os
import argparse
import json
from tqdm.notebook import tqdm
import pandas as pd
import seaborn as sns
from IPython.display import display, HTML

from datasets import load_dataset
import tiktoken

# Set up a results directory for storing experiment results and visualizations
os.makedirs("results", exist_ok=True)

# Create a small test file for quick experimentation
with open('3seqs.txt', 'w') as f:
    f.write("this is a simple test sequence for the model to learn\n")
    f.write("another example sequence that is a bit different from the first one\n")
    f.write("machine learning models are interesting to build and experiment with\n")

# Define experiment configuration structure
class ExperimentConfig:
    """Configuration class for Pico-LLM experiments"""
    def __init__(self, **kwargs):
        # Data parameters
        self.input_files = kwargs.get("input_files", [])
        self.tinystories_weight = kwargs.get("tinystories_weight", 0.95)  # Increased weight for TinyStories
        self.train_subset_size = kwargs.get("train_subset_size", 15000)   # Increased dataset size
        self.block_size = kwargs.get("block_size", 128)                   # Increased context window

        # Model parameters
        self.model_type = kwargs.get("model_type", "all")  # "kgram", "lstm", "transformer", or "all"
        self.embed_size = kwargs.get("embed_size", 256)    # Increased embedding size
        self.kgram_k = kwargs.get("kgram_k", 5)            # Increased k-gram window
        self.kgram_chunk_size = kwargs.get("kgram_chunk_size", 2)  # More efficient processing
        self.num_inner_mlp_layers = kwargs.get("num_inner_mlp_layers", 2)  # More complexity
        self.use_positional_embedding = kwargs.get("use_positional_embedding", "absolute")  # Enable positional embeddings

        # Training parameters
        self.batch_size = kwargs.get("batch_size", 32)      # Increased batch size
        self.num_epochs = kwargs.get("num_epochs", 8)       # More training epochs
        self.learning_rate = kwargs.get("learning_rate", 5e-4)  # Adjusted learning rate
        self.max_steps_per_epoch = kwargs.get("max_steps_per_epoch", 100)  # More steps per epoch
        self.device_id = kwargs.get("device_id", "cuda" if torch.cuda.is_available() else "cpu")

        # Text generation
        self.prompt = kwargs.get("prompt", "Once upon a time, there was a")  # Better prompt

        # Logging
        self.log_interval_steps = kwargs.get("log_interval_steps", 10)
        self.sample_interval_seconds = kwargs.get("sample_interval_seconds", 60)  # Less frequent sampling

        # Optional tasks
        self.run_overfitting_study = kwargs.get("run_overfitting_study", False)
        self.run_hyperparameter_study = kwargs.get("run_hyperparameter_study", False)
        self.monosemantic_enabled = kwargs.get("monosemantic_enabled", False)

        # Experiment name
        self.experiment_name = kwargs.get("experiment_name",
                                         f"{self.model_type}_e{self.embed_size}_lr{self.learning_rate}_k{self.kgram_k}")

    def as_dict(self):
        """Convert config to dictionary for easy serialization"""
        return self.__dict__

    def __str__(self):
        """Pretty string representation of config"""
        return json.dumps(self.as_dict(), indent=2)

# Create an experiment tracker for comparing results
class ExperimentTracker:
    """Tracks experiments, logs results, and creates visualizations"""
    def __init__(self, root_dir="results"):
        self.root_dir = root_dir
        self.experiments = {}
        os.makedirs(root_dir, exist_ok=True)

    def add_experiment(self, config):
        """Register a new experiment"""
        exp_dir = os.path.join(self.root_dir, config.experiment_name)
        os.makedirs(exp_dir, exist_ok=True)

        # Save configuration
        with open(os.path.join(exp_dir, "config.json"), "w") as f:
            json.dump(config.as_dict(), f, indent=2)

        self.experiments[config.experiment_name] = {
            "config": config,
            "training_history": [],
            "generated_samples": [],
        }

        return config.experiment_name

    def log_training_step(self, exp_name, step, loss, global_step=None):
        """Log a training step result"""
        if exp_name not in self.experiments:
            raise ValueError(f"Experiment {exp_name} not found")

        self.experiments[exp_name]["training_history"].append({
            "step": step,
            "global_step": global_step,
            "loss": loss
        })

    def log_generated_sample(self, exp_name, prompt, generated_text, sampling_method):
        """Log a generated text sample"""
        if exp_name not in self.experiments:
            raise ValueError(f"Experiment {exp_name} not found")

        self.experiments[exp_name]["generated_samples"].append({
            "prompt": prompt,
            "text": generated_text,
            "method": sampling_method,
            "timestamp": time.time()
        })

    def plot_training_history(self, exp_names=None, save=True):
        """Plot training loss history for one or more experiments"""
        if exp_names is None:
            exp_names = list(self.experiments.keys())
        elif isinstance(exp_names, str):
            exp_names = [exp_names]

        plt.figure(figsize=(12, 6))

        for exp_name in exp_names:
            if exp_name not in self.experiments:
                print(f"Warning: Experiment {exp_name} not found, skipping")
                continue

            history = self.experiments[exp_name]["training_history"]
            if not history:
                print(f"Warning: No training history for {exp_name}, skipping")
                continue

            steps = [entry["step"] for entry in history]
            losses = [entry["loss"] for entry in history]

            plt.plot(steps, losses, label=exp_name)

        plt.xlabel("Training Step")
        plt.ylabel("Loss")
        plt.title("Training Loss Comparison")
        plt.legend()
        plt.grid(True, alpha=0.3)

        if save:
            save_path = os.path.join(self.root_dir, "training_comparison.png")
            plt.savefig(save_path, dpi=300)
            print(f"Saved training comparison plot to {save_path}")

        plt.show()

    def compare_samples(self, exp_names=None, sampling_method=None):
        """Display generated samples side by side for comparison"""
        if exp_names is None:
            exp_names = list(self.experiments.keys())
        elif isinstance(exp_names, str):
            exp_names = [exp_names]

        # Build HTML for comparison table
        html = "<table style='width:100%; border-collapse: collapse;'>"
        html += "<tr><th style='border:1px solid black; padding: 8px;'>Experiment</th>"
        html += "<th style='border:1px solid black; padding: 8px;'>Prompt</th>"
        html += "<th style='border:1px solid black; padding: 8px;'>Generated Text</th>"
        html += "<th style='border:1px solid black; padding: 8px;'>Sampling Method</th></tr>"

        for exp_name in exp_names:
            if exp_name not in self.experiments:
                continue

            samples = self.experiments[exp_name]["generated_samples"]
            if not samples:
                continue

            # Get the most recent sample of each sampling method
            method_samples = {}
            for sample in samples:
                if sampling_method is not None and sample["method"] != sampling_method:
                    continue
                method_samples[sample["method"]] = sample

            for method, sample in method_samples.items():
                html += f"<tr><td style='border:1px solid black; padding: 8px;'>{exp_name}</td>"
                html += f"<td style='border:1px solid black; padding: 8px;'>{sample['prompt']}</td>"
                html += f"<td style='border:1px solid black; padding: 8px;'>{sample['text']}</td>"
                html += f"<td style='border:1px solid black; padding: 8px;'>{method}</td></tr>"

        html += "</table>"
        display(HTML(html))

    def save_comparison_report(self, exp_names=None):
        """Generate a comprehensive comparison report"""
        if exp_names is None:
            exp_names = list(self.experiments.keys())
        elif isinstance(exp_names, str):
            exp_names = [exp_names]

        report_path = os.path.join(self.root_dir, "comparison_report.md")

        with open(report_path, "w") as f:
            f.write("# Pico-LLM Experiment Comparison Report\n\n")

            # Configurations
            f.write("## Model Configurations\n\n")
            f.write("| Experiment | Model Type | Embed Size | Context Size | Learning Rate |\n")
            f.write("|------------|------------|------------|--------------|---------------|\n")

            for exp_name in exp_names:
                if exp_name not in self.experiments:
                    continue

                config = self.experiments[exp_name]["config"]
                f.write(f"| {exp_name} | {config.model_type} | {config.embed_size} | {config.kgram_k} | {config.learning_rate} |\n")

            # Training Results
            f.write("\n## Training Results\n\n")
            f.write("| Experiment | Final Loss | Training Steps |\n")
            f.write("|------------|------------|----------------|\n")

            for exp_name in exp_names:
                if exp_name not in self.experiments:
                    continue

                history = self.experiments[exp_name]["training_history"]
                if not history:
                    continue

                final_loss = history[-1]["loss"]
                steps = len(history)

                f.write(f"| {exp_name} | {final_loss:.4f} | {steps} |\n")

            # Generated Samples
            f.write("\n## Generated Text Samples\n\n")

            for exp_name in exp_names:
                if exp_name not in self.experiments:
                    continue

                samples = self.experiments[exp_name]["generated_samples"]
                if not samples:
                    continue

                f.write(f"### {exp_name}\n\n")

                # Group by sampling method
                method_samples = {}
                for sample in samples:
                    if sample["method"] not in method_samples:
                        method_samples[sample["method"]] = []
                    method_samples[sample["method"]].append(sample)

                for method, samples_list in method_samples.items():
                    # Get the most recent sample for each method
                    sample = max(samples_list, key=lambda x: x["timestamp"])

                    f.write(f"**{method}**\n\n")
                    f.write(f"Prompt: \"{sample['prompt']}\"\n\n")
                    f.write(f"```\n{sample['text']}\n```\n\n")

        print(f"Saved comparison report to {report_path}")

    def __str__(self):
        return f"ExperimentTracker with {len(self.experiments)} experiments"

# Create global experiment tracker
tracker = ExperimentTracker()

################################################################################
# Data handling: entire sequences up to block_size => (seq_len, batch)
################################################################################

class MixedSequenceDataset(torch.utils.data.Dataset):
    """
    We store two lists of entire token sequences:
      - tinystories_seqs
      - other_seqs
    Each sequence is length <= block_size.

    During __getitem__, we randomly pick from one list or the other with probability p_tiny.
    Return that entire sequence as a 1D LongTensor.
    """
    def __init__(self, tinystories_seqs, other_seqs, p_tiny: float):
        super().__init__()
        self.tinystories_seqs = tinystories_seqs
        self.other_seqs = other_seqs
        self.p_tiny = p_tiny

        self.has_tinystories = (len(self.tinystories_seqs) > 0)
        self.has_other = (len(self.other_seqs) > 0)

        self.total_length = len(self.tinystories_seqs) + len(self.other_seqs)
        if self.total_length == 0:
            raise ValueError("No data found! Both TinyStories and other sets are empty.")

    def __len__(self):
        return self.total_length

    def __getitem__(self, idx):
        r = random.random()
        if self.has_tinystories and self.has_other:
            if r < self.p_tiny:
                i = random.randint(0, len(self.tinystories_seqs) - 1)
                seq = self.tinystories_seqs[i]
            else:
                i = random.randint(0, len(self.other_seqs) - 1)
                seq = self.other_seqs[i]
        elif self.has_tinystories:
            i = random.randint(0, len(self.tinystories_seqs) - 1)
            seq = self.tinystories_seqs[i]
        else:
            i = random.randint(0, len(self.other_seqs) - 1)
            seq = self.other_seqs[i]

        return torch.tensor(seq, dtype=torch.long)


def seq_collate_fn(batch):
    """
    batch: list of 1D LongTensors of various lengths [<= block_size].
    1) find max length
    2) pad with zeros
    3) shape => (max_len, batch_size)
    """
    max_len = max(len(seq) for seq in batch)
    batch_size = len(batch)

    padded = torch.zeros(max_len, batch_size, dtype=torch.long)
    for i, seq in enumerate(batch):
        seq_len = seq.size(0)
        padded[:seq_len, i] = seq

    return padded


################################################################################
# K-gram MLP in a sequence-to-sequence approach
################################################################################

def compute_next_token_loss(logits, tokens):
    """
    logits: (seq_len, batch, vocab_size)
    tokens: (seq_len, batch)
    Next-token prediction => we shift target by 1.
    """
    seq_len, batch_size, vocab_size = logits.shape
    if seq_len < 2:
        return torch.tensor(0.0, device=logits.device, requires_grad=True)

    preds = logits[:-1, :, :]  # (seq_len-1, batch, vocab_size)
    gold = tokens[1:, :]       # (seq_len-1, batch)

    preds = preds.reshape(-1, vocab_size)
    gold = gold.reshape(-1)
    return F.cross_entropy(preds, gold)


class KGramMLPSeqModel(nn.Module):
    """
    For each position t in [0..seq_len-1], gather the last k tokens => one-hot => MLP => logits.
    Return (seq_len, batch, vocab_size).

    Potentially very large memory usage for big vocab or seq_len. chunk_size helps mitigate overhead.
    """

    def __init__(self, vocab_size, k=3, embed_size=1024, num_inner_layers=1, chunk_size=1):
        super().__init__()
        self.k = k
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.num_inner_layers = num_inner_layers
        self.chunk_size = chunk_size

        # IMPLEMENTATION: Using embeddings for efficiency
        self.embedding = nn.Embedding(vocab_size, embed_size)

        # Create the MLP layers
        layers = []

        # Input layer: k embeddings concatenated
        layers.append(nn.Linear(k * embed_size, embed_size))
        layers.append(nn.SiLU())  # SiLU activation (Swish)

        # Inner layers
        for _ in range(num_inner_layers):
            layers.append(nn.Linear(embed_size, embed_size))
            layers.append(nn.SiLU())

        # Output layer
        layers.append(nn.Linear(embed_size, vocab_size))

        # Wrap as sequential module
        self.mlp = nn.Sequential(*layers)

        # Define the forward function for self.net
        def forward_with_embedding(x):
            # x is a flattened one-hot tensor (batch, k*vocab_size)
            batch_size = x.shape[0]

            # Reshape to recover token indices from one-hot encoding
            x_reshaped = x.view(batch_size, self.k, self.vocab_size)
            indices = torch.argmax(x_reshaped, dim=2)  # (batch, k)

            # Use embedding instead of one-hot
            embedded = self.embedding(indices)  # (batch, k, embed_size)

            # Flatten for MLP input
            flat_embed = embedded.reshape(batch_size, -1)  # (batch, k*embed_size)

            # Process through MLP
            return self.mlp(flat_embed)

        # Set self.net as required by the code
        self.net = forward_with_embedding

    def forward(self, tokens_seq):
        """
        tokens_seq: (seq_len, batch)
        return: (seq_len, batch, vocab_size)
        We'll do a loop over time steps. chunk_size can reduce overhead.
        """
        seq_len, batch_size = tokens_seq.shape
        outputs = []

        start = 0
        while start < seq_len:
            end = min(start + self.chunk_size, seq_len)
            block_outputs = []
            for t in range(start, end):
                batch_logits = []
                for b in range(batch_size):
                    if t < self.k:
                        needed = self.k - t
                        context_ids = [0]*needed + tokens_seq[:t, b].tolist()
                    else:
                        context_ids = tokens_seq[t-self.k:t, b].tolist()

                    context_oh = F.one_hot(
                        torch.tensor(context_ids, dtype=torch.long, device=tokens_seq.device),
                        num_classes=self.vocab_size
                    )
                    context_flat = context_oh.flatten().float().unsqueeze(0)
                    logits_b = self.net(context_flat)  # (1, vocab_size)
                    batch_logits.append(logits_b)
                block_outputs.append(torch.cat(batch_logits, dim=0).unsqueeze(0))  # (1, batch, vocab_size)

            block_outputs = torch.cat(block_outputs, dim=0)  # (chunk_size, batch, vocab_size)
            outputs.append(block_outputs)
            start = end

        outputs = torch.cat(outputs, dim=0)  # (seq_len, batch, vocab_size)
        return outputs


################################################################################
# LSTM-based seq2seq
################################################################################

class LSTMSeqModel(nn.Module):
    def __init__(self, vocab_size, embed_size=1024, hidden_size=1024):
        super().__init__()
        self.vocab_size = vocab_size
        self.embed_size = embed_size
        self.hidden_size = hidden_size

        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=False)
        self.linear = nn.Linear(hidden_size, vocab_size)

    def forward(self, tokens_seq):
        """
        tokens_seq: (seq_len, batch)
        => (seq_len, batch, vocab_size)
        """
        emb = self.embedding(tokens_seq)   # (seq_len, batch, embed)
        self.lstm.flatten_parameters()
        out, _ = self.lstm(emb)           # (seq_len, batch, hidden)
        logits = self.linear(out)         # (seq_len, batch, vocab_size)
        return logits


################################################################################
# Our Transformer with implementation
################################################################################

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))
    def forward(self, x):
        norm = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()
        return self.weight * (x / norm)

class AbsolutePositionalEmbedding(nn.Module):
    """
    Simple sinusoidal positional embeddings from the original Transformer paper.
    Adds positional information to token embeddings.
    """
    def __init__(self, d_model, max_seq_len=1024):
        super().__init__()
        # Create a positional encoding matrix of shape (max_seq_len, d_model)
        pe = torch.zeros(max_seq_len, d_model)

        # Create position vector (max_seq_len, 1)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)

        # Create div_term to compute sin/cos at different frequencies
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Apply sin to even indices and cos to odd indices
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # Add batch dimension and register as a buffer (not a parameter)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: Tensor of shape (batch_size, seq_len, d_model)
        """
        # Add positional encoding to input embeddings (only up to seq_len positions)
        x = x + self.pe[:, :x.size(1), :]
        return x

class TransformerModel(nn.Module):
    def __init__(self, vocab_size=50257, d_model=1024, n_heads=8, n_blocks=6, use_pos_emb=True):
        super().__init__()
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_blocks = n_blocks
        self.use_pos_emb = use_pos_emb

        # Token embedding
        self.embedding = nn.Embedding(vocab_size, d_model)

        # Optional positional embedding
        if use_pos_emb:
            self.positional_embedding = AbsolutePositionalEmbedding(d_model)

        # Transformer blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads)
            for _ in range(n_blocks)
        ])

        # Final normalization
        self.norm = RMSNorm(d_model)

        # Output projection to vocabulary
        self.lm_head = nn.Linear(d_model, vocab_size)

    def forward(self, tokens_seq):
        """
        tokens_seq: (seq_len, batch)
        return: (seq_len, batch, vocab_size)
        """
        # Transpose to batch-first: (batch, seq_len)
        x = tokens_seq.transpose(0, 1)

        # Embedding: (batch, seq_len, d_model)
        x = self.embedding(x)

        # Add positional embeddings if enabled
        if self.use_pos_emb:
            x = self.positional_embedding(x)

        # Process through transformer blocks
        for block in self.blocks:
            x = block(x)

        # Final normalization
        x = self.norm(x)

        # Project to vocabulary
        logits = self.lm_head(x)  # (batch, seq_len, vocab_size)

        # Transpose back to match expected output
        logits = logits.transpose(0, 1)  # (seq_len, batch, vocab_size)

        return logits


class TransformerBlock(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        # Pre-normalization for attention (following Llama-style architecture)
        self.norm1 = RMSNorm(d_model)

        # Multi-head attention
        self.attention = MultiHeadAttention(d_model, n_heads)

        # Pre-normalization for feed-forward
        self.norm2 = RMSNorm(d_model)

        # Feed-forward network
        self.ffn = FeedForward(d_model)

    def forward(self, x):
        # Pre-norm with residual connection for attention
        x = x + self.attention(self.norm1(x))

        # Pre-norm with residual connection for feed-forward
        x = x + self.ffn(self.norm2(x))

        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        # Fix: Calculate head_dim dynamically to avoid assertion error
        self.head_dim = d_model // n_heads

        # Linear projections for query, key, value for all heads
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)

        # Output projection
        self.o_proj = nn.Linear(d_model, d_model)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # Project to query, key, value
        q = self.q_proj(x)  # (batch, seq, d_model)
        k = self.k_proj(x)  # (batch, seq, d_model)
        v = self.v_proj(x)  # (batch, seq, d_model)

        # Reshape to multiple heads
        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # (batch, n_heads, seq, head_dim)
        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # (batch, n_heads, seq, head_dim)
        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)  # (batch, n_heads, seq, head_dim)

        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (batch, n_heads, seq, seq)

        # Apply causal (autoregressive) mask - only attend to past tokens
        mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()
        scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))

        # Apply softmax
        attn_weights = F.softmax(scores, dim=-1)  # (batch, n_heads, seq, seq)

        # Apply attention to values
        context = torch.matmul(attn_weights, v)  # (batch, n_heads, seq, head_dim)

        # Reshape back
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (batch, seq, d_model)

        # Final projection
        output = self.o_proj(context)  # (batch, seq, d_model)

        return output


class FeedForward(nn.Module):
    def __init__(self, d_model, expansion_factor=4):
        super().__init__()
        hidden_dim = d_model * expansion_factor

        self.net = nn.Sequential(
            nn.Linear(d_model, hidden_dim),
            nn.SiLU(),  # SiLU activation as used in modern transformers
            nn.Linear(hidden_dim, d_model)
        )

    def forward(self, x):
        return self.net(x)


################################################################################
# Text generation with nucleus sampling
################################################################################

def nucleus_sampling(logits, p=0.85, temperature=0.8):
    """
    Implements nucleus sampling (top-p sampling).

    1. Convert logits to probabilities
    2. Sort tokens by probability (descending)
    3. Keep the smallest set of tokens whose cumulative probability >= p
    4. Sample from this subset based on their relative probabilities

    Args:
        logits: tensor of shape (vocab_size,)
        p: probability threshold (default 0.85)
        temperature: controls randomness (default 0.8)

    Returns:
        sampled token id
    """
    # Apply temperature scaling
    logits = logits / temperature

    # Convert logits to probabilities
    probs = F.softmax(logits, dim=-1)

    # Sort probabilities in descending order
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)

    # Calculate cumulative probabilities
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)

    # Find indices where cumulative probability > p
    sorted_indices_to_remove = cumulative_probs > p

    # Shift to include the first token exceeding p
    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
    sorted_indices_to_remove[0] = False  # Always keep at least one token

    # Filter the distribution
    filtered_indices = sorted_indices[~sorted_indices_to_remove]
    filtered_probs = sorted_probs[~sorted_indices_to_remove]

    # Renormalize probabilities
    filtered_probs = filtered_probs / filtered_probs.sum()

    # Sample from the filtered distribution
    sample_idx = torch.multinomial(filtered_probs, 1).item()

    # Return the corresponding token
    return filtered_indices[sample_idx].item()


def generate_text(model, enc, init_text, max_new_tokens=80, device="cpu",
                  top_p=None, temperature=0.8, verbose=True):
    """
    A single code path for all models:
      - We keep a growing list 'context_tokens'.
      - At each step, we feed the entire context as (seq_len,1) to model(...).
      - We get model(...)->(seq_len,1,vocab_size). We take the final step's logits => logits[-1,0,:].
      - We pick next token (greedy or top-p), append to context_tokens.

    Args:
        model: The language model
        enc: Tokenizer encoder
        init_text: Initial prompt text
        max_new_tokens: Maximum number of tokens to generate
        device: Computing device
        top_p: If None, use greedy decoding; otherwise use nucleus sampling with this p value
        temperature: Controls randomness (lower = more deterministic)
        verbose: If True, print generation progress

    Returns:
        final_text: The complete generated text (prompt + new tokens)
        tokens_generated: List of tokens that were generated
        token_probabilities: List of probabilities for each generated token
    """
    was_training = model.training
    model.eval()

    tokens_generated = []
    token_probabilities = []

    with torch.no_grad():
        context_tokens = enc.encode(init_text)

        if verbose:
            print(f"Prompt: \"{init_text}\"")
            print("Generating", end="", flush=True)

        for step_i in range(max_new_tokens):
            seq_tensor = torch.tensor(context_tokens, dtype=torch.long, device=device).unsqueeze(1)
            logits_seq = model(seq_tensor)              # (seq_len,1,vocab_size)
            next_logits = logits_seq[-1, 0, :]         # shape (vocab_size,)

            # Calculate probabilities for visualization
            probs = F.softmax(next_logits / temperature, dim=-1)

            if top_p is None:
                # greedy
                chosen_token = torch.argmax(next_logits).item()
                chosen_prob = probs[chosen_token].item()
            else:
                chosen_token = nucleus_sampling(next_logits, p=top_p, temperature=temperature)
                chosen_prob = probs[chosen_token].item()

            context_tokens.append(chosen_token)
            tokens_generated.append(chosen_token)
            token_probabilities.append(chosen_prob)

            if verbose and step_i % 5 == 0:
                print(".", end="", flush=True)

    if verbose:
        print("\nGeneration complete!")

    model.train(was_training)

    final_text = enc.decode(context_tokens)

    return final_text, tokens_generated, token_probabilities


################################################################################
# 8. Training and evaluation functions
################################################################################

def train_one_model(model,
                    loader,
                    epochs,
                    model_name,
                    device,
                    lr=5e-4,
                    log_steps=10,
                    sample_interval=60,
                    max_steps_per_epoch=100,
                    enc=None,
                    prompt="Once upon a time, there was a",
                    experiment_tracker=None,
                    exp_name=None,
                    verbose=True):
    """
    Train a model and track the results

    Args:
        model: The model to train
        loader: DataLoader for training
        epochs: Number of epochs to train
        model_name: Name of the model architecture for logging
        device: Computing device
        lr: Learning rate
        log_steps: How often to log progress
        sample_interval: How often to generate sample text
        max_steps_per_epoch: Maximum number of steps per epoch
        enc: Tokenizer encoder
        prompt: Prompt for text generation
        experiment_tracker: ExperimentTracker instance for logging results
        exp_name: Name of the experiment in the tracker
        verbose: Whether to print detailed progress

    Returns:
        average_loss: The average loss over the last epoch
    """
    optimizer = optim.Adam(model.parameters(), lr=lr)

    start_time = time.time()
    next_sample_time = start_time
    global_step = 0

    # Progress bar theme
    progress_bar_format = '{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'

    # Keep track of losses for visualization
    all_losses = []

    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0.0
        partial_loss = 0.0
        partial_count = 0

        step_in_epoch = 0

        # Create progress bar
        pbar = tqdm(total=min(len(loader), max_steps_per_epoch or float('inf')),
                   desc=f"Epoch {epoch}/{epochs}",
                   bar_format=progress_bar_format,
                   leave=True)

        for batch_idx, batch_tokens in enumerate(loader, start=1):
            step_in_epoch += 1
            global_step += 1

            batch_tokens = batch_tokens.to(device)  # (seq_len, batch)

            logits = model(batch_tokens)  # (seq_len, batch, vocab_size)
            loss = compute_next_token_loss(logits, batch_tokens)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            loss_value = loss.item()
            total_loss += loss_value
            partial_loss += loss_value
            partial_count += 1
            all_losses.append(loss_value)

            # Track loss in experiment tracker
            if experiment_tracker is not None and exp_name is not None:
                experiment_tracker.log_training_step(exp_name, step_in_epoch, loss_value, global_step)

            if batch_idx % log_steps == 0:
                avg_part_loss = partial_loss / partial_count

                # Update progress bar with loss info
                pbar.set_postfix({"loss": f"{avg_part_loss:.4f}"})

                if verbose:
                    print(f"\n[{model_name}] Epoch {epoch}/{epochs}, "
                          f"Step {batch_idx}/{len(loader)} (global step: {global_step}) "
                          f"Partial Avg Loss: {avg_part_loss:.4f}")

                partial_loss = 0.0
                partial_count = 0

            current_time = time.time()
            if current_time >= next_sample_time and enc is not None:
                with torch.no_grad():
                    if verbose:
                        print(f"\n[{model_name}] Generating sample text (greedy) at epoch={epoch}, step={batch_idx}...")

                    text_greedy, tokens_greedy, probs_greedy = generate_text(
                        model, enc, prompt, max_new_tokens=40, device=device,
                        top_p=None, temperature=0.8, verbose=False
                    )

                    if verbose:
                        print(f" Greedy Sample: {text_greedy}")

                    # Track generated text
                    if experiment_tracker is not None and exp_name is not None:
                        experiment_tracker.log_generated_sample(exp_name, prompt, text_greedy, "greedy")

                    if verbose:
                        print(f"\n[{model_name}] Generating sample text (top-p=0.85) at epoch={epoch}, step={batch_idx}...")

                    text_topp, tokens_topp, probs_topp = generate_text(
                        model, enc, prompt, max_new_tokens=40, device=device,
                        top_p=0.85, temperature=0.8, verbose=False
                    )

                    if verbose:
                        print(f" Top-p (p=0.85) Sample: {text_topp}")

                    # Track generated text
                    if experiment_tracker is not None and exp_name is not None:
                        experiment_tracker.log_generated_sample(exp_name, prompt, text_topp, "top-p=0.85")

                next_sample_time = current_time + sample_interval

            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:
                if verbose:
                    print(f"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, ending epoch {epoch} early.")
                break

            # Update progress bar
            pbar.update(1)

        # Close progress bar for the epoch
        pbar.close()

        avg_loss = total_loss / step_in_epoch
        if verbose:
            print(f"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}")

    # Plot the loss curve
    plt.figure(figsize=(10, 5))
    plt.plot(all_losses)
    plt.title(f"{model_name} Training Loss")
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.grid(True, alpha=0.3)
    plt.show()

    # Generate final samples
    with torch.no_grad():
        print(f"\n[{model_name}] Final text generation examples:")

        print("\nGreedy sampling:")
        text_greedy, tokens_greedy, probs_greedy = generate_text(
            model, enc, prompt, max_new_tokens=80, device=device,
            top_p=None, temperature=0.8, verbose=False
        )
        print(text_greedy)

        print("\nNucleus sampling (p=0.85):")
        text_topp, tokens_topp, probs_topp = generate_text(
            model, enc, prompt, max_new_tokens=80, device=device,
            top_p=0.85, temperature=0.8, verbose=False
        )
        print(text_topp)

    return avg_loss

################################################################################
# 9. Experiment functions for various studies
################################################################################

def run_architecture_comparison(config, tracker):
    """Compare different model architectures with the same hyperparameters"""
    print("\n" + "="*80)
    print("ARCHITECTURE COMPARISON EXPERIMENT")
    print("="*80)

    # Get data and tokenizer
    enc, train_loader, vocab_size = load_dataset_and_tokenizer(config)
    device = torch.device(config.device_id)

    # Create models to compare
    models = {
        "kgram": KGramMLPSeqModel(
            vocab_size=vocab_size,
            k=config.kgram_k,
            embed_size=config.embed_size,
            num_inner_layers=config.num_inner_mlp_layers,
            chunk_size=config.kgram_chunk_size
        ).to(device),

        "lstm": LSTMSeqModel(
            vocab_size=vocab_size,
            embed_size=config.embed_size,
            hidden_size=config.embed_size
        ).to(device),

        "transformer": TransformerModel(
            vocab_size=vocab_size,
            d_model=config.embed_size,
            n_heads=8,                # Increased from 4 to 8
            n_blocks=6,               # Increased from 4 to 6
            use_pos_emb=True          # Enforced to True
        ).to(device)
    }

    # Train each model and track results
    results = {}
    for model_name, model in models.items():
        print(f"\n{'='*40}\nTraining {model_name.upper()} model\n{'='*40}")

        # Configuration for this experiment
        exp_config = ExperimentConfig(
            model_type=model_name,
            embed_size=config.embed_size,
            kgram_k=config.kgram_k,
            learning_rate=config.learning_rate,
            experiment_name=f"architecture_comparison_{model_name}"
        )

        # Register experiment
        exp_name = tracker.add_experiment(exp_config)

        # Count parameters
        num_params = sum(p.numel() for p in model.parameters())
        print(f"Model has {num_params:,} parameters")

        # Train model
        start_time = time.time()
        final_loss = train_one_model(
            model=model,
            loader=train_loader,
            epochs=config.num_epochs,
            model_name=model_name,
            device=device,
            lr=config.learning_rate,
            log_steps=config.log_interval_steps,
            sample_interval=config.sample_interval_seconds,
            max_steps_per_epoch=config.max_steps_per_epoch,
            enc=enc,
            prompt=config.prompt,
            experiment_tracker=tracker,
            exp_name=exp_name
        )
        train_time = time.time() - start_time

        # Store results
        results[model_name] = {
            "final_loss": final_loss,
            "train_time": train_time,
            "num_parameters": num_params
        }

    # Plot comparison results
    plt.figure(figsize=(12, 5))

    models_list = list(results.keys())
    losses = [results[m]["final_loss"] for m in models_list]
    train_times = [results[m]["train_time"] / 60 for m in models_list]  # Convert to minutes
    num_params = [results[m]["num_parameters"] / 1_000_000 for m in models_list]  # Convert to millions

    # Create bar plot with 3 metrics
    x = np.arange(len(models_list))
    width = 0.25

    fig, ax1 = plt.subplots(figsize=(12, 6))

    # Plot bars for loss
    rects1 = ax1.bar(x - width, losses, width, label='Final Loss', color='skyblue')
    ax1.set_ylabel('Loss', color='blue')
    ax1.tick_params(axis='y', labelcolor='blue')

    # Create second y-axis for training time
    ax2 = ax1.twinx()
    rects2 = ax2.bar(x, train_times, width, label='Training Time (minutes)', color='salmon')
    ax2.set_ylabel('Training Time (minutes)', color='red')
    ax2.tick_params(axis='y', labelcolor='red')

    # Create third y-axis for parameters
    ax3 = ax1.twinx()
    ax3.spines['right'].set_position(('outward', 60))
    rects3 = ax3.bar(x + width, num_params, width, label='Parameters (millions)', color='lightgreen')
    ax3.set_ylabel('Parameters (millions)', color='green')
    ax3.tick_params(axis='y', labelcolor='green')

    # Add labels and title
    ax1.set_xlabel('Model Architecture')
    ax1.set_title('Comparison of Model Architectures')
    ax1.set_xticks(x)
    ax1.set_xticklabels(models_list)

    # Create combined legend
    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    lines3, labels3 = ax3.get_legend_handles_labels()
    ax1.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, loc='upper left')

    fig.tight_layout()
    plt.savefig("results/architecture_comparison.png", dpi=300)
    plt.show()

    # Compare generated text
    print("\nText generation comparison:")
    tracker.compare_samples([f"architecture_comparison_{m}" for m in models_list])

    return results

def run_kgram_context_study(config, tracker):
    """Study the effect of k parameter in k-gram models"""
    print("\n" + "="*80)
    print("K-GRAM CONTEXT WINDOW STUDY")
    print("="*80)

    # Get data and tokenizer
    enc, train_loader, vocab_size = load_dataset_and_tokenizer(config)
    device = torch.device(config.device_id)

    # Define k values to test
    k_values = [1, 2, 3, 5, 8]

    # Train models with different k values
    results = {}
    for k in k_values:
        print(f"\n{'='*40}\nTraining K-Gram model with k={k}\n{'='*40}")

        # Configuration for this experiment
        exp_config = ExperimentConfig(
            model_type="kgram",
            kgram_k=k,
            embed_size=config.embed_size,
            experiment_name=f"kgram_context_study_k{k}"
        )

        # Register experiment
        exp_name = tracker.add_experiment(exp_config)

        # Create model
        model = KGramMLPSeqModel(
            vocab_size=vocab_size,
            k=k,
            embed_size=config.embed_size,
            num_inner_layers=config.num_inner_mlp_layers,
            chunk_size=config.kgram_chunk_size
        ).to(device)

        # Count parameters
        num_params = sum(p.numel() for p in model.parameters())
        print(f"Model has {num_params:,} parameters")

        # Train model
        start_time = time.time()
        final_loss = train_one_model(
            model=model,
            loader=train_loader,
            epochs=config.num_epochs,
            model_name=f"kgram_k{k}",
            device=device,
            lr=config.learning_rate,
            log_steps=config.log_interval_steps,
            sample_interval=config.sample_interval_seconds,
            max_steps_per_epoch=config.max_steps_per_epoch,
            enc=enc,
            prompt=config.prompt,
            experiment_tracker=tracker,
            exp_name=exp_name
        )
        train_time = time.time() - start_time

        # Store results
        results[k] = {
            "final_loss": final_loss,
            "train_time": train_time,
            "num_parameters": num_params
        }

    # Plot comparison results
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Values for plotting
    k_list = list(results.keys())
    losses = [results[k]["final_loss"] for k in k_list]
    train_times = [results[k]["train_time"] / 60 for k in k_list]  # Convert to minutes

    # Plot loss vs k
    ax1.plot(k_list, losses, marker='o', linestyle='-', linewidth=2, color='blue')
    ax1.set_xlabel('Context Window Size (k)')
    ax1.set_ylabel('Final Loss')
    ax1.set_title('Effect of Context Window Size on K-Gram Performance')
    ax1.grid(True, alpha=0.3)

    # Plot training time vs k
    ax2.plot(k_list, train_times, marker='s', linestyle='-', linewidth=2, color='red')
    ax2.set_xlabel('Context Window Size (k)')
    ax2.set_ylabel('Training Time (minutes)')
    ax2.set_title('Effect of Context Window Size on Training Time')
    ax2.grid(True, alpha=0.3)

    fig.tight_layout()
    plt.savefig("results/kgram_context_study.png", dpi=300)
    plt.show()

    # Compare generated text
    print("\nText generation comparison across different context window sizes:")
    tracker.compare_samples([f"kgram_context_study_k{k}" for k in k_list])

    return results

def run_positional_embedding_study(config, tracker):
    """Compare transformers with and without positional embeddings"""
    print("\n" + "="*80)
    print("POSITIONAL EMBEDDING STUDY")
    print("="*80)

    # Get data and tokenizer
    enc, train_loader, vocab_size = load_dataset_and_tokenizer(config)
    device = torch.device(config.device_id)

    # Create models with and without positional embeddings
    models = {
        "no_pos_emb": TransformerModel(
            vocab_size=vocab_size,
            d_model=config.embed_size,
            n_heads=8,
            n_blocks=6,
            use_pos_emb=False
        ).to(device),

        "with_pos_emb": TransformerModel(
            vocab_size=vocab_size,
            d_model=config.embed_size,
            n_heads=8,
            n_blocks=6,
            use_pos_emb=True
        ).to(device)
    }

    # Train both models and track results
    results = {}
    for model_name, model in models.items():
        print(f"\n{'='*40}\nTraining transformer {model_name}\n{'='*40}")

        # Configuration for this experiment
        exp_config = ExperimentConfig(
            model_type="transformer",
            embed_size=config.embed_size,
            use_positional_embedding="absolute" if "with_pos" in model_name else "none",
            experiment_name=f"positional_embedding_study_{model_name}"
        )

        # Register experiment
        exp_name = tracker.add_experiment(exp_config)

        # Train model
        start_time = time.time()
        final_loss = train_one_model(
            model=model,
            loader=train_loader,
            epochs=config.num_epochs,
            model_name=model_name,
            device=device,
            lr=config.learning_rate,
            log_steps=config.log_interval_steps,
            sample_interval=config.sample_interval_seconds,
            max_steps_per_epoch=config.max_steps_per_epoch,
            enc=enc,
            prompt=config.prompt,
            experiment_tracker=tracker,
            exp_name=exp_name
        )
        train_time = time.time() - start_time

        # Store results
        results[model_name] = {
            "final_loss": final_loss,
            "train_time": train_time
        }

    # Test the models on specific position-sensitive prompts
    position_test_prompts = [
        "Count from 1 to 10:",
        "The days of the week are: Monday,",
        "The alphabet starts with: a, b, c,",
        "The first five prime numbers are: 2, 3, 5,",
        "Complete the pattern: A B A B A"
    ]

    # Generate completions for each prompt
    for prompt in position_test_prompts:
        print(f"\n{'='*40}\nTesting prompt: \"{prompt}\"\n{'='*40}")

        for model_name, model in models.items():
            with torch.no_grad():
                text, _, _ = generate_text(
                    model, enc, prompt, max_new_tokens=40, device=device,
                    top_p=0.85, temperature=0.8, verbose=False
                )
                print(f"\n{model_name}: {text}")

                # Log to tracker
                tracker.log_generated_sample(
                    f"positional_embedding_study_{model_name}",
                    prompt, text, "position_test"
                )

    # Compare results
    print("\nComparing positional embedding results on position-sensitive tasks:")
    tracker.compare_samples([f"positional_embedding_study_{m}" for m in models.keys()],
                          sampling_method="position_test")

    return results

def run_nucleus_sampling_study(config, tracker):
    """Study the effect of top-p parameter in nucleus sampling"""
    print("\n" + "="*80)
    print("NUCLEUS SAMPLING PARAMETER STUDY")
    print("="*80)

    # Get data and tokenizer
    enc, train_loader, vocab_size = load_dataset_and_tokenizer(config)
    device = torch.device(config.device_id)

    # Use a single transformer model for this study
    model = TransformerModel(
        vocab_size=vocab_size,
        d_model=config.embed_size,
        n_heads=8,
        n_blocks=6,
        use_pos_emb=True
    ).to(device)

    # Configuration for this experiment
    exp_config = ExperimentConfig(
        model_type="transformer",
        embed_size=config.embed_size,
        use_positional_embedding=config.use_positional_embedding,
        experiment_name="nucleus_sampling_study"
    )

    # Register experiment
    exp_name = tracker.add_experiment(exp_config)

    # Train the model
    print(f"\n{'='*40}\nTraining transformer model for sampling study\n{'='*40}")
    final_loss = train_one_model(
        model=model,
        loader=train_loader,
        epochs=config.num_epochs,
        model_name="transformer",
        device=device,
        lr=config.learning_rate,
        log_steps=config.log_interval_steps,
        sample_interval=config.sample_interval_seconds,
        max_steps_per_epoch=config.max_steps_per_epoch,
        enc=enc,
        prompt=config.prompt,
        experiment_tracker=tracker,
        exp_name=exp_name
    )

    # Test different p values and temperatures
    p_values = [0.0, 0.5, 0.7, 0.85, 0.95, 1.0]  # p=0.0 is effectively greedy
    temperatures = [0.6, 0.8, 1.0, 1.2]  # Added temperature variations

    test_prompts = [
        "Once upon a time",
        "The solution to this problem is",
        "In the future, artificial intelligence will",
        "The best way to learn programming is"
    ]

    all_samples = []

    # Generate completions for each prompt with each p value
    for prompt in test_prompts:
        print(f"\n{'='*40}\nTesting prompt: \"{prompt}\"\n{'='*40}")

        # First, test different p values with fixed temperature
        for p in p_values:
            method_name = "greedy" if p == 0.0 else f"top-p={p}"

            with torch.no_grad():
                text, tokens, probs = generate_text(
                    model, enc, prompt, max_new_tokens=80, device=device,
                    top_p=p if p > 0 else None, temperature=0.8, verbose=False
                )
                print(f"\n{method_name}:\n{text}")

                # Log to tracker
                tracker.log_generated_sample(
                    exp_name, prompt, text, method_name
                )

                all_samples.append({
                    "prompt": prompt,
                    "p_value": p,
                    "temperature": 0.8,
                    "method_name": method_name,
                    "text": text,
                    "avg_probability": sum(probs) / len(probs),
                    "min_probability": min(probs),
                    "token_entropy": -(sum([p * math.log(p) for p in probs]) / len(probs))
                })

        # Then, test different temperatures with fixed p-value
        for temp in temperatures:
            method_name = f"temp={temp}"
            if temp != 0.8:  # Skip 0.8 as it was already tested above
                with torch.no_grad():
                    text, tokens, probs = generate_text(
                        model, enc, prompt, max_new_tokens=80, device=device,
                        top_p=0.85, temperature=temp, verbose=False
                    )
                    print(f"\n{method_name}:\n{text}")

                    # Log to tracker
                    tracker.log_generated_sample(
                        exp_name, prompt, text, method_name
                    )

                    all_samples.append({
                        "prompt": prompt,
                        "p_value": 0.85,
                        "temperature": temp,
                        "method_name": method_name,
                        "text": text,
                        "avg_probability": sum(probs) / len(probs),
                        "min_probability": min(probs),
                        "token_entropy": -(sum([p * math.log(p) for p in probs]) / len(probs))
                    })

    # Analyze text diversity metrics
    diversity_df = pd.DataFrame(all_samples)

    # Visualize how p-value affects token probabilities and entropy
    plt.figure(figsize=(15, 12))

    # Plot p-value effects
    plt.subplot(2, 2, 1)
    p_df = diversity_df[diversity_df['temperature'] == 0.8]
    sns.boxplot(x='p_value', y='avg_probability', data=p_df)
    plt.title('Effect of p-value on Token Probabilities')
    plt.xlabel('p-value')
    plt.ylabel('Average Token Probability')

    plt.subplot(2, 2, 2)
    sns.boxplot(x='p_value', y='token_entropy', data=p_df)
    plt.title('Effect of p-value on Token Entropy')
    plt.xlabel('p-value')
    plt.ylabel('Token Entropy')

    # Plot temperature effects
    plt.subplot(2, 2, 3)
    temp_df = diversity_df[diversity_df['p_value'] == 0.85]
    sns.boxplot(x='temperature', y='avg_probability', data=temp_df)
    plt.title('Effect of Temperature on Token Probabilities')
    plt.xlabel('Temperature')
    plt.ylabel('Average Token Probability')

    plt.subplot(2, 2, 4)
    sns.boxplot(x='temperature', y='token_entropy', data=temp_df)
    plt.title('Effect of Temperature on Token Entropy')
    plt.xlabel('Temperature')
    plt.ylabel('Token Entropy')

    plt.tight_layout()
    plt.savefig("results/nucleus_sampling_study.png", dpi=300)
    plt.show()

    # Compare text samples
    print("\nComparing nucleus sampling with different p values:")
    tracker.compare_samples([exp_name])

    return diversity_df

def run_embedding_size_study(config, tracker):
    """Study the effect of embedding size on model performance"""
    print("\n" + "="*80)
    print("EMBEDDING SIZE STUDY")
    print("="*80)

    # Get data and tokenizer
    enc, train_loader, vocab_size = load_dataset_and_tokenizer(config)
    device = torch.device(config.device_id)

    # Define embedding sizes to test
    embed_sizes = [32, 64, 128, 256]

    # Test on all model architectures
    model_types = ["kgram", "lstm", "transformer"]

    # Store results for all combinations
    results = {}

    for model_type in model_types:
        results[model_type] = {}

        for embed_size in embed_sizes:
            print(f"\n{'='*40}\nTraining {model_type} with embedding size {embed_size}\n{'='*40}")

            # Configuration for this experiment
            exp_config = ExperimentConfig(
                model_type=model_type,
                embed_size=embed_size,
                kgram_k=config.kgram_k,
                experiment_name=f"embedding_size_study_{model_type}_e{embed_size}"
            )

            # Register experiment
            exp_name = tracker.add_experiment(exp_config)

            # Create appropriate model based on type
            if model_type == "kgram":
                model = KGramMLPSeqModel(
                    vocab_size=vocab_size,
                    k=config.kgram_k,
                    embed_size=embed_size,
                    num_inner_layers=config.num_inner_mlp_layers,
                    chunk_size=config.kgram_chunk_size
                ).to(device)
            elif model_type == "lstm":
                model = LSTMSeqModel(
                    vocab_size=vocab_size,
                    embed_size=embed_size,
                    hidden_size=embed_size
                ).to(device)
            elif model_type == "transformer":
                # Scale heads with embedding size
                n_heads = min(8, max(1, embed_size // 32))
                model = TransformerModel(
                    vocab_size=vocab_size,
                    d_model=embed_size,
                    n_heads=n_heads,
                    n_blocks=6,
                    use_pos_emb=True
                ).to(device)

            # Count parameters
            num_params = sum(p.numel() for p in model.parameters())
            print(f"Model has {num_params:,} parameters")

            # Train model
            start_time = time.time()
            final_loss = train_one_model(
                model=model,
                loader=train_loader,
                epochs=config.num_epochs,
                model_name=f"{model_type}_e{embed_size}",
                device=device,
                lr=config.learning_rate,
                log_steps=config.log_interval_steps,
                sample_interval=config.sample_interval_seconds,
                max_steps_per_epoch=config.max_steps_per_epoch,
                enc=enc,
                prompt=config.prompt,
                experiment_tracker=tracker,
                exp_name=exp_name,
                verbose=False  # Reduce verbosity for this experiment
            )
            train_time = time.time() - start_time

            # Store results
            results[model_type][embed_size] = {
                "final_loss": final_loss,
                "train_time": train_time,
                "num_parameters": num_params
            }

    # Plot comparison results
    plt.figure(figsize=(15, 10))

    # Plot loss vs embedding size for each model type
    plt.subplot(2, 2, 1)
    for model_type in model_types:
        embed_list = list(results[model_type].keys())
        losses = [results[model_type][e]["final_loss"] for e in embed_list]
        plt.plot(embed_list, losses, marker='o', linestyle='-', linewidth=2, label=model_type)

    plt.xlabel('Embedding Size')
    plt.ylabel('Final Loss')
    plt.title('Effect of Embedding Size on Model Performance')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xscale('log', base=2)

    # Plot training time vs embedding size
    plt.subplot(2, 2, 2)
    for model_type in model_types:
        embed_list = list(results[model_type].keys())
        times = [results[model_type][e]["train_time"] / 60 for e in embed_list]  # Convert to minutes
        plt.plot(embed_list, times, marker='s', linestyle='-', linewidth=2, label=model_type)

    plt.xlabel('Embedding Size')
    plt.ylabel('Training Time (minutes)')
    plt.title('Effect of Embedding Size on Training Time')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xscale('log', base=2)

    # Plot parameters vs embedding size
    plt.subplot(2, 2, 3)
    for model_type in model_types:
        embed_list = list(results[model_type].keys())
        params = [results[model_type][e]["num_parameters"] / 1_000_000 for e in embed_list]  # Convert to millions
        plt.plot(embed_list, params, marker='^', linestyle='-', linewidth=2, label=model_type)

    plt.xlabel('Embedding Size')
    plt.ylabel('Parameters (millions)')
    plt.title('Effect of Embedding Size on Model Size')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xscale('log', base=2)

    # Plot performance per parameter
    plt.subplot(2, 2, 4)
    for model_type in model_types:
        embed_list = list(results[model_type].keys())
        efficiency = [results[model_type][e]["final_loss"] * results[model_type][e]["num_parameters"] / 1_000_000
                     for e in embed_list]
        plt.plot(embed_list, efficiency, marker='*', linestyle='-', linewidth=2, label=model_type)

    plt.xlabel('Embedding Size')
    plt.ylabel('Loss × Parameters (millions)')
    plt.title('Parameter Efficiency vs Embedding Size')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xscale('log', base=2)

    plt.tight_layout()
    plt.savefig("results/embedding_size_study.png", dpi=300)
    plt.show()

    # Compare generated text samples
    print("\nComparing text generation with different embedding sizes:")

    for model_type in model_types:
        print(f"\n{model_type.upper()} model text generation with different embedding sizes:")
        tracker.compare_samples([f"embedding_size_study_{model_type}_e{e}" for e in embed_sizes])

    return results

def run_overfitting_study(config, tracker):
    """Study overfitting behavior with train/validation splits"""
    print("\n" + "="*80)
    print("OVERFITTING STUDY")
    print("="*80)

    # Get data and tokenizer
    tinystories_seqs, other_seqs, enc, vocab_size = prepare_data(config)
    device = torch.device(config.device_id)

    # Create train/validation split
    all_seqs = tinystories_seqs + other_seqs
    random.shuffle(all_seqs)

    split_idx = int(0.8 * len(all_seqs))
    train_seqs = all_seqs[:split_idx]
    val_seqs = all_seqs[split_idx:]

    print(f"Split data into {len(train_seqs)} training sequences and {len(val_seqs)} validation sequences")

    # Create custom datasets instead of TensorDataset
    train_dataset = MixedSequenceDataset(
        tinystories_seqs=train_seqs,
        other_seqs=[],  # No other sequences, all are in train_seqs
        p_tiny=1.0      # Always sample from train_seqs
    )

    val_dataset = MixedSequenceDataset(
        tinystories_seqs=val_seqs,
        other_seqs=[],  # No other sequences, all are in val_seqs
        p_tiny=1.0      # Always sample from val_seqs
    )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,
        collate_fn=seq_collate_fn
    )

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=seq_collate_fn
    )

    # Create models to test
    models = {
        "kgram": KGramMLPSeqModel(
            vocab_size=vocab_size,
            k=config.kgram_k,
            embed_size=config.embed_size,
            num_inner_layers=config.num_inner_mlp_layers,
            chunk_size=config.kgram_chunk_size
        ).to(device),

        "transformer": TransformerModel(
            vocab_size=vocab_size,
            d_model=config.embed_size,
            n_heads=8,
            n_blocks=6,
            use_pos_emb=True
        ).to(device)
    }

    # Train and track results
    results = {}
    for model_name, model in models.items():
        print(f"\n{'='*40}\nTraining {model_name} for overfitting study\n{'='*40}")

        # Configuration for this experiment
        exp_config = ExperimentConfig(
            model_type=model_name,
            embed_size=config.embed_size,
            kgram_k=config.kgram_k,
            num_epochs=10,  # More epochs to observe overfitting
            experiment_name=f"overfitting_study_{model_name}"
        )

        # Register experiment
        exp_name = tracker.add_experiment(exp_config)

        # Initialize optimizer
        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)

        # Track losses
        train_losses = []
        val_losses = []

        # Train for multiple epochs
        for epoch in range(1, 10 + 1):  # 10 epochs to observe overfitting
            # Training phase
            model.train()
            total_train_loss = 0.0
            train_steps = 0

            pbar = tqdm(total=min(len(train_loader), config.max_steps_per_epoch or float('inf')),
                       desc=f"Epoch {epoch}/10 (Train)",
                       leave=True)

            for batch_idx, batch_tokens in enumerate(train_loader, start=1):
                if config.max_steps_per_epoch and batch_idx > config.max_steps_per_epoch:
                    break

                batch_tokens = batch_tokens.to(device)

                # Forward pass
                logits = model(batch_tokens)
                loss = compute_next_token_loss(logits, batch_tokens)

                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                loss_value = loss.item()
                total_train_loss += loss_value
                train_steps += 1

                pbar.update(1)
                pbar.set_postfix({"loss": f"{loss_value:.4f}"})

            pbar.close()

            # Calculate average training loss
            avg_train_loss = total_train_loss / train_steps
            train_losses.append(avg_train_loss)

            # Validation phase
            model.eval()
            total_val_loss = 0.0
            val_steps = 0

            pbar = tqdm(total=min(len(val_loader), config.max_steps_per_epoch // 2 or float('inf')),
                       desc=f"Epoch {epoch}/10 (Val)",
                       leave=True)

            with torch.no_grad():
                for batch_idx, batch_tokens in enumerate(val_loader, start=1):
                    if config.max_steps_per_epoch and batch_idx > config.max_steps_per_epoch // 2:
                        break

                    batch_tokens = batch_tokens.to(device)

                    # Forward pass
                    logits = model(batch_tokens)
                    loss = compute_next_token_loss(logits, batch_tokens)

                    loss_value = loss.item()
                    total_val_loss += loss_value
                    val_steps += 1

                    pbar.update(1)
                    pbar.set_postfix({"loss": f"{loss_value:.4f}"})

            pbar.close()

            # Calculate average validation loss
            avg_val_loss = total_val_loss / val_steps
            val_losses.append(avg_val_loss)

            # Log results
            print(f"Epoch {epoch}/10 - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, "
                  f"Gap: {avg_val_loss - avg_train_loss:.4f}")

            # Generate sample text
            if epoch % 2 == 0:
                with torch.no_grad():
                    text, _, _ = generate_text(
                        model, enc, config.prompt, max_new_tokens=80, device=device,
                        top_p=0.85, temperature=0.8, verbose=False
                    )

                    print(f"\nEpoch {epoch} sample text:\n{text}\n")

                    # Log to tracker
                    tracker.log_generated_sample(
                        exp_name, config.prompt, text, f"epoch_{epoch}"
                    )

        # Store results
        results[model_name] = {
            "train_losses": train_losses,
            "val_losses": val_losses
        }

        # Plot training curves
        plt.figure(figsize=(10, 6))
        epochs = range(1, len(train_losses) + 1)

        plt.plot(epochs, train_losses, 'b-', label='Training Loss')
        plt.plot(epochs, val_losses, 'r-', label='Validation Loss')

        plt.title(f'Training and Validation Loss - {model_name}')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Add the gap between train and val
        gaps = [v - t for v, t in zip(val_losses, train_losses)]
        plt.plot(epochs, gaps, 'g--', label='Gap (Val - Train)')

        plt.legend()
        plt.savefig(f"results/overfitting_study_{model_name}.png", dpi=300)
        plt.show()

    # Compare final training curves across models
    plt.figure(figsize=(12, 10))

    plt.subplot(2, 1, 1)
    for model_name, result in results.items():
        plt.plot(range(1, len(result["train_losses"]) + 1),
                result["train_losses"],
                marker='o', linestyle='-', label=f'{model_name} - Train')
        plt.plot(range(1, len(result["val_losses"]) + 1),
                result["val_losses"],
                marker='s', linestyle='--', label=f'{model_name} - Val')

    plt.title('Training and Validation Loss Comparison')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 1, 2)
    for model_name, result in results.items():
        gaps = [v - t for v, t in zip(result["val_losses"], result["train_losses"])]
        plt.plot(range(1, len(gaps) + 1),
                gaps,
                marker='*', linestyle='-', label=f'{model_name} - Gap')

    plt.title('Overfitting Gap (Val Loss - Train Loss)')
    plt.xlabel('Epochs')
    plt.ylabel('Gap')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("results/overfitting_comparison.png", dpi=300)
    plt.show()

    # Compare text generation
    print("\nText generation across training epochs:")
    for model_name in models.keys():
        print(f"\n{model_name.upper()} model text generation evolution:")
        tracker.compare_samples([f"overfitting_study_{model_name}"])

    return results

def prepare_data(config):
    """Load and prepare data based on configuration"""
    tinystories_seqs = []
    other_seqs = []

    # Initialize tokenizer
    enc = tiktoken.get_encoding("gpt2")
    vocab_size = enc.n_vocab
    print(f"Using tokenizer with vocabulary size: {vocab_size}")

    # Load TinyStories if weight > 0
    if config.tinystories_weight > 0.0:
        print(f"Loading TinyStories from huggingface with weight={config.tinystories_weight}...")
        dataset = load_dataset("roneneldan/TinyStories", split="train")
        dataset = dataset.select(range(min(config.train_subset_size, len(dataset))))

        for sample in tqdm(dataset, desc="Processing TinyStories"):
            text = sample['text']
            tokens = enc.encode(text)
            tokens = tokens[:config.block_size]
            if len(tokens) > 0:
                tinystories_seqs.append(tokens)

        print(f"Loaded {len(tinystories_seqs)} sequences from TinyStories")

    # Load custom input files
    if config.input_files:
        for filepath in config.input_files:
            print(f"Reading custom text file: {filepath}")
            with open(filepath, "r", encoding="utf-8") as f:
                lines = f.readlines()

            for line in tqdm(lines, desc=f"Processing {filepath}"):
                line = line.strip()
                if not line:
                    continue
                tokens = enc.encode(line)
                tokens = tokens[:config.block_size]
                if len(tokens) > 0:
                    other_seqs.append(tokens)

            print(f"Loaded {len(other_seqs)} sequences from custom files")

    print(f"Total data: {len(tinystories_seqs) + len(other_seqs)} sequences")

    return tinystories_seqs, other_seqs, enc, vocab_size

def load_dataset_and_tokenizer(config):
    """Load data and create dataset and dataloader"""
    tinystories_seqs, other_seqs, enc, vocab_size = prepare_data(config)

    # Create dataset and dataloader
    dataset = MixedSequenceDataset(
        tinystories_seqs=tinystories_seqs,
        other_seqs=other_seqs,
        p_tiny=config.tinystories_weight
    )

    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,
        collate_fn=seq_collate_fn
    )

    print(f"Created dataloader with {len(dataset)} sequences")

    return enc, dataloader, vocab_size

def print_model_summary(model):
    """Print a summary of model parameters and architecture"""
    print(f"\nModel Architecture: {model.__class__.__name__}")

    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # Print layer-by-layer breakdown
    print("\nLayer-by-layer breakdown:")
    for name, module in model.named_children():
        params = sum(p.numel() for p in module.parameters())
        print(f"  {name}: {params:,} parameters")

        # If it's a container, show its children
        if isinstance(module, (nn.Sequential, nn.ModuleList)):
            for idx, layer in enumerate(module):
                layer_params = sum(p.numel() for p in layer.parameters())
                print(f"    {idx}: {layer.__class__.__name__}: {layer_params:,} parameters")

    print("\n")


################################################################################
# Main function with options to run different experiments
################################################################################

def main():
    # Let the user choose experiments to run
    print("\n" + "="*80)
    print("PICO-LLM EXPERIMENTS")
    print("="*80)
    print("\nChoose experiments to run:")
    print("1. Basic model training (all architectures)")
    print("2. K-gram context window study")
    print("3. Transformer positional embedding study")
    print("4. Nucleus sampling parameter study")
    print("5. Embedding size comparative study")
    print("6. Overfitting study")
    print("7. Run all experiments")

    choice = input("\nEnter your choice (1-7): ")

    # Set up improved base configuration
    base_config = ExperimentConfig(
        # Data parameters
        input_files=["3seqs.txt"],
        tinystories_weight=0.95,              # Increase TinyStories weight (more coherent data)
        train_subset_size=15000,              # Increase training data
        block_size=128,                       # Increase context length

        # Model parameters
        embed_size=256,                       # Larger embeddings for transformers
        kgram_k=5,                            # Larger context window for k-gram
        kgram_chunk_size=2,                   # More efficient k-gram processing
        num_inner_mlp_layers=2,               # More complexity for k-gram
        use_positional_embedding="absolute",  # Critical for transformer coherence

        # Training parameters
        batch_size=32,                        # Increased batch size
        num_epochs=8,                         # More training epochs
        learning_rate=5e-4,                   # Slightly lower learning rate
        max_steps_per_epoch=100,              # More steps per epoch

        # Text generation parameters
        prompt="Once upon a time, there was a",  # More specific prompt

        # Experiment parameters
        sample_interval_seconds=60,           # Sample less frequently

        # Device selection
        device_id="cuda" if torch.cuda.is_available() else "cpu"
    )

    # Create experiment tracker
    global tracker
    tracker = ExperimentTracker(root_dir="results")

    if choice == "1" or choice == "7":
        # Basic model training
        run_architecture_comparison(base_config, tracker)

    if choice == "2" or choice == "7":
        # K-gram context window study
        run_kgram_context_study(base_config, tracker)

    if choice == "3" or choice == "7":
        # Transformer positional embedding study
        run_positional_embedding_study(base_config, tracker)

    if choice == "4" or choice == "7":
        # Nucleus sampling parameter study
        run_nucleus_sampling_study(base_config, tracker)

    if choice == "5" or choice == "7":
        # Embedding size study
        run_embedding_size_study(base_config, tracker)

    if choice == "6" or choice == "7":
        # Overfitting study
        run_overfitting_study(base_config, tracker)

    # Generate final comprehensive report
    tracker.plot_training_history()
    tracker.save_comparison_report()

    print("\n" + "="*80)
    print("EXPERIMENTS COMPLETE")
    print("="*80)
    print(f"\nResults and visualizations saved to the '{tracker.root_dir}' directory")
    print("Check the comparison_report.md file for a comprehensive analysis")

if __name__ == "__main__":
    main()